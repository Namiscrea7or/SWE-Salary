{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import concurrent.futures\n",
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Site: levels.fyi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_job_data(company_id: str, job_title_id: str, size: str):\n",
    "    url = f'https://www.levels.fyi/company/{company_id}/salaries/{job_title_id}'\n",
    "    driver = webdriver.Chrome()\n",
    "    driver.get(url)\n",
    "    print(f'--> crawling {company_id} {job_title_id}...')\n",
    "\n",
    "    records = []\n",
    "\n",
    "    try:\n",
    "        button = driver.find_element(\n",
    "            by=By.CSS_SELECTOR, value=\"button.MuiButton-root.MuiButton-text.MuiButton-textPrimary.MuiButton-sizeMedium.MuiButton-textSizeMedium.MuiButtonBase-root.css-um5318\")\n",
    "        button.click()\n",
    "\n",
    "        button = driver.find_element(\n",
    "            by=By.CSS_SELECTOR, value=\"button.MuiButton-root.MuiButton-text.MuiButton-textPrimary.MuiButton-sizeMedium.MuiButton-textSizeMedium.MuiButtonBase-root.css-g9gvkf\")\n",
    "        button.click()\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    try:\n",
    "        tbody = driver.find_elements(by=By.CSS_SELECTOR, value=\"tbody.MuiTableBody-root\")[1]\n",
    "        rows = tbody.find_elements(by=By.TAG_NAME, value=\"tr\")\n",
    "\n",
    "        for row in rows:\n",
    "            try:\n",
    "                cells = row.find_elements(by=By.TAG_NAME, value=\"td\")\n",
    "\n",
    "                location = cells[0].find_element(by=By.TAG_NAME, value=\"span\").find_element(by=By.TAG_NAME, value=\"span\").text.split('|')[0].strip()\n",
    "                level = cells[1].find_element(by=By.TAG_NAME, value=\"p\").text\n",
    "                field = cells[1].find_element(by=By.TAG_NAME, value=\"span\").text\n",
    "                yoe_total = cells[2].find_element(by=By.TAG_NAME, value=\"p\").text\n",
    "                yoe_at_company = cells[2].find_element(by=By.TAG_NAME, value=\"span\").text\n",
    "                total_compensation = cells[3].find_element(by=By.TAG_NAME, value=\"p\").text\n",
    "                compensation_details = cells[3].find_element(by=By.TAG_NAME, value=\"span\").text.split('|')\n",
    "                base = compensation_details[0].strip()\n",
    "                stock = compensation_details[1].strip()\n",
    "                bonus = compensation_details[2].strip()\n",
    "\n",
    "                new_record = {\n",
    "                    'company': company_id.replace('-', ' ').title(),\n",
    "                    'company_size': size,\n",
    "                    'job_title': job_title_id.replace('-', ' ').title(),\n",
    "                    'level': level,\n",
    "                    'domain': field,\n",
    "                    'yoe_total': yoe_total,\n",
    "                    'yoe_at_company': yoe_at_company,\n",
    "                    'base': base,\n",
    "                    'stock': stock,\n",
    "                    'bonus': bonus,\n",
    "                    'total_compensation': total_compensation,\n",
    "                    'location': location\n",
    "                }\n",
    "\n",
    "                records.append(new_record)\n",
    "            except:\n",
    "                continue\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    driver.close()\n",
    "    return pd.DataFrame(records)\n",
    "\n",
    "\n",
    "def crawl_company(company_id: str):\n",
    "    page = requests.get(f'https://www.levels.fyi/companies/{company_id}')\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "    size = soup.findAll(\n",
    "        'h6', class_='MuiTypography-root MuiTypography-subtitle1 css-idrr7q')[1].text\n",
    "\n",
    "    page = requests.get(\n",
    "        f'https://www.levels.fyi/companies/{company_id}/salaries')\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "    job_titles_container = soup.find_all(\n",
    "        'h6', class_='MuiTypography-root MuiTypography-h6 css-jv9qtm')\n",
    "    job_titles = [job_title.text for job_title in job_titles_container]\n",
    "    job_title_ids = [job_title.strip().lower().replace(' ', '-') for job_title in job_titles]\n",
    "    \n",
    "    threads = []\n",
    "    scraped_dfs = []\n",
    "\n",
    "    # Use ThreadPoolExecutor to run the scraping function in parallel\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=5) as executor:\n",
    "        for job_title_id in job_title_ids:\n",
    "            # Pass df, company_id, job_title_id, and size as arguments to the scraping function\n",
    "            thread = executor.submit(scrape_job_data, company_id, job_title_id, size)\n",
    "            threads.append(thread)\n",
    "\n",
    "    # Wait for all threads to finish\n",
    "    concurrent.futures.wait(threads)\n",
    "\n",
    "    # Retrieve the results from the threads\n",
    "    for thread in threads:\n",
    "        try:\n",
    "            df_result = thread.result()\n",
    "            scraped_dfs.append(df_result)\n",
    "        except Exception as e:\n",
    "            print(f\"Error retrieving result from thread: {e}\")\n",
    "\n",
    "    # Concatenate all DataFrames into a single DataFrame\n",
    "    result_df = pd.concat(scraped_dfs, ignore_index=True)\n",
    "\n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Popular companies (but they are also the top-paying to me)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Google',\n",
       " 'Amazon',\n",
       " 'Apple',\n",
       " 'Lyft',\n",
       " 'Facebook',\n",
       " 'Microsoft',\n",
       " 'Uber',\n",
       " 'Stripe',\n",
       " 'Roblox',\n",
       " 'Coinbase',\n",
       " 'Databricks',\n",
       " 'Snap',\n",
       " 'Netflix',\n",
       " 'LinkedIn',\n",
       " 'Salesforce',\n",
       " 'Hudson River Trading',\n",
       " 'Jane Street',\n",
       " 'Citadel',\n",
       " 'Two Sigma',\n",
       " 'JPMorgan Chase',\n",
       " 'Capital One',\n",
       " 'Oracle',\n",
       " 'Bytedance',\n",
       " 'Intel']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "page = requests.get('https://www.levels.fyi/companies')\n",
    "soup = BeautifulSoup(page.content, 'html.parser')\n",
    "popular_companies_container = soup.find_all(\n",
    "    'h6', class_='MuiTypography-root MuiTypography-h6 css-1v6gvkr')\n",
    "popular_companies = [company.text for company in popular_companies_container]\n",
    "popular_companies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We'd like to put the desired companies here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Crawled companies:\n",
    "- Logitech\n",
    "- Microsoft\n",
    "- Netflix\n",
    "- Uber\n",
    "- Visa\n",
    "- Google\n",
    "- Apple\n",
    "- JPMorgan Chase\n",
    "- Shopee\n",
    "- Tiki\n",
    "- Grab\n",
    "- Gojek Tech\n",
    "- Riot Games\n",
    "- LinkedIn\n",
    "- Intel\n",
    "- MongoDB\n",
    "- Roblox\n",
    "- Oracle\n",
    "- Stripe\n",
    "- Facebook\n",
    "- Amazon\n",
    "- Snap\n",
    "- Axon\n",
    "- AMD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure to check if the url exists: https://www.levels.fyi/company/{company-name}\n",
    "desired_companies = ['company-name']\n",
    "\n",
    "desired_companies_df = pd.DataFrame()\n",
    "\n",
    "for desired_company in desired_companies:\n",
    "\tdesired_company_df = crawl_company(desired_company)\n",
    "\tdesired_companies_df = pd.concat([desired_companies_df, desired_company_df], ignore_index=True)\n",
    "\n",
    "desired_companies_df.to_csv('./data/desired_companies.csv', index=False)\n",
    "\n",
    "desired_companies_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
