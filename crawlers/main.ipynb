{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import concurrent.futures\n",
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_df = pd.DataFrame(columns=[\n",
    "\t'company',\n",
    "\t'company_size',\n",
    "\t'job_title',\n",
    "\t'level',\n",
    "\t'domain',\n",
    "\t'yoe_total',\n",
    "\t'yoe_at_company',\n",
    "\t'total_compensation',\n",
    "\t'location'\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Site: levels.fyi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_job_data(company_id: str, job_title_id: str, size: str):\n",
    "    url = f'https://www.levels.fyi/company/{company_id}/salaries/{job_title_id}'\n",
    "    driver = webdriver.Chrome()\n",
    "    driver.get(url)\n",
    "    print(f'--> crawling {company_id} {job_title_id}...')\n",
    "\n",
    "    records = []\n",
    "\n",
    "    try:\n",
    "        button = driver.find_element(\n",
    "            by=By.CSS_SELECTOR, value=\"button.MuiButton-root.MuiButton-text.MuiButton-textPrimary.MuiButton-sizeMedium.MuiButton-textSizeMedium.MuiButtonBase-root.css-um5318\")\n",
    "        button.click()\n",
    "\n",
    "        button = driver.find_element(\n",
    "            by=By.CSS_SELECTOR, value=\"button.MuiButton-root.MuiButton-text.MuiButton-textPrimary.MuiButton-sizeMedium.MuiButton-textSizeMedium.MuiButtonBase-root.css-g9gvkf\")\n",
    "        button.click()\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    try:\n",
    "        tbody = driver.find_elements(by=By.CSS_SELECTOR, value=\"tbody.MuiTableBody-root\")[1]\n",
    "        rows = tbody.find_elements(by=By.TAG_NAME, value=\"tr\")\n",
    "\n",
    "        for row in rows:\n",
    "            try:\n",
    "                cells = row.find_elements(by=By.TAG_NAME, value=\"td\")\n",
    "\n",
    "                location = cells[0].find_element(by=By.TAG_NAME, value=\"span\").find_element(by=By.TAG_NAME, value=\"span\").text.split('|')[0].strip()\n",
    "                level = cells[1].find_element(by=By.TAG_NAME, value=\"p\").text\n",
    "                field = cells[1].find_element(by=By.TAG_NAME, value=\"span\").text\n",
    "                yoe_total = cells[2].find_element(by=By.TAG_NAME, value=\"p\").text\n",
    "                yoe_at_company = cells[2].find_element(by=By.TAG_NAME, value=\"span\").text\n",
    "                total_compensation = cells[3].find_element(by=By.TAG_NAME, value=\"p\").text\n",
    "\n",
    "                new_record = {\n",
    "                    'company': company_id,\n",
    "                    'company_size': size,\n",
    "                    'job_title': job_title_id.replace('-', ' ').title(),\n",
    "                    'level': level,\n",
    "                    'domain': field,\n",
    "                    'yoe_total': yoe_total,\n",
    "                    'yoe_at_company': yoe_at_company,\n",
    "                    'total_compensation': total_compensation,\n",
    "                    'location': location\n",
    "                }\n",
    "\n",
    "                records.append(new_record)\n",
    "            except:\n",
    "                continue\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    driver.close()\n",
    "    return pd.DataFrame(records)\n",
    "\n",
    "\n",
    "def crawl_company(company_id: str):\n",
    "    page = requests.get(f'https://www.levels.fyi/companies/{company_id}')\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "    size = soup.findAll(\n",
    "        'h6', class_='MuiTypography-root MuiTypography-subtitle1 css-idrr7q')[1].text\n",
    "\n",
    "    page = requests.get(\n",
    "        f'https://www.levels.fyi/companies/{company_id}/salaries')\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "    job_titles_container = soup.find_all(\n",
    "        'h6', class_='MuiTypography-root MuiTypography-h6 css-jv9qtm')\n",
    "    job_titles = [job_title.text for job_title in job_titles_container]\n",
    "    job_title_ids = [job_title.strip().lower().replace(' ', '-') for job_title in job_titles]\n",
    "    \n",
    "    threads = []\n",
    "    scraped_dfs = []\n",
    "\n",
    "    # Use ThreadPoolExecutor to run the scraping function in parallel\n",
    "    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "        for job_title_id in job_title_ids:\n",
    "            # Pass df, company_id, job_title_id, and size as arguments to the scraping function\n",
    "            thread = executor.submit(scrape_job_data, company_id, job_title_id, size)\n",
    "            threads.append(thread)\n",
    "\n",
    "    # Wait for all threads to finish\n",
    "    concurrent.futures.wait(threads)\n",
    "\n",
    "    # Retrieve the results from the threads\n",
    "    for thread in threads:\n",
    "        try:\n",
    "            df_result = thread.result()\n",
    "            scraped_dfs.append(df_result)\n",
    "        except Exception as e:\n",
    "            print(f\"Error retrieving result from thread: {e}\")\n",
    "\n",
    "    # Concatenate all DataFrames into a single DataFrame\n",
    "    result_df = pd.concat(scraped_dfs, ignore_index=True)\n",
    "\n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.DataFrame(columns=[\n",
    "\t'company',\n",
    "\t'company_size',\n",
    "\t'job_title',\n",
    "\t'level',\n",
    "\t'domain',\n",
    "\t'yoe_total',\n",
    "\t'yoe_at_company',\n",
    "\t'total_compensation',\n",
    "\t'location'\n",
    "])\n",
    "\n",
    "test_df = scrape_job_data('shopee', 'software-engineer', '10000+')\n",
    "\n",
    "test_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Popular companies (but they are also the top-paying to me)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "page = requests.get('https://www.levels.fyi/companies')\n",
    "soup = BeautifulSoup(page.content, 'html.parser')\n",
    "popular_companies_container = soup.find_all(\n",
    "    'h6', class_='MuiTypography-root MuiTypography-h6 css-1v6gvkr')\n",
    "popular_companies = [company.text for company in popular_companies_container]\n",
    "popular_companies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "popular_companies_df = pd.DataFrame(columns=[\n",
    "\t'company',\n",
    "\t'company_size',\n",
    "\t'job_title',\n",
    "\t'level',\n",
    "\t'domain',\n",
    "\t'yoe_total',\n",
    "\t'yoe_at_company',\n",
    "\t'total_compensation',\n",
    "\t'location'\n",
    "])\n",
    "\n",
    "for popular_company in popular_companies:\n",
    "\tpopular_companies_df = pd.concat(\n",
    "\t\t[popular_companies_df, crawl_company(popular_company)], ignore_index=True)\n",
    "\n",
    "popular_companies_df.to_csv('popular_companies.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We'd like to put the desired companies here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "desired_companies = [\n",
    "\t'shopee',\n",
    "\t'tiki',\n",
    "\t'grab',\n",
    "\t'gojek-tech',\n",
    "\t'kms-technology',\n",
    "\t'riot-games',\n",
    "]\n",
    "\n",
    "desired_companies_df = pd.DataFrame(columns=[\n",
    "\t'company',\n",
    "\t'company_size',\n",
    "\t'job_title',\n",
    "\t'level',\n",
    "\t'domain',\n",
    "\t'yoe_total',\n",
    "\t'yoe_at_company',\n",
    "\t'total_compensation',\n",
    "\t'location'\n",
    "])\n",
    "\n",
    "for desired_company in desired_companies:\n",
    "\tdesired_company_df = crawl_company(desired_company)\n",
    "\tdesired_companies_df = pd.concat([desired_companies_df, desired_company_df], ignore_index=True)\n",
    "\n",
    "desired_companies_df.to_csv('desired_companies.csv', index=False)\n",
    "\n",
    "desired_companies_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grouping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([popular_companies_df, desired_companies_df])\n",
    "df.to_csv('all_companies.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
